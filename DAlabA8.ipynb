{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf77a25-e52a-40a1-903e-2f4b6a06f753",
   "metadata": {},
   "source": [
    "# DA5401 A8: Ensemble Learning for Complex Regression Modeling on Bike Share Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b951429-0903-4c69-8b64-181081d0c3c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "To apply and compare **single-model** and **ensemble learning** techniques for predicting hourly bike rentals (`cnt`) using the **UCI Bike Sharing Dataset**. Also demonstrate your understanding of how these methods address model variance and bias, and how a diverse stack of models can yield superior performance to any single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e76335-052c-4486-a901-a675a277a7e1",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945d19d-d098-4f16-82a3-b9c2314a4b43",
   "metadata": {},
   "source": [
    "### A.1 Data Loading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc2c947-c747-44e4-b356-cec3ecef4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (17379, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"   \n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "df = pd.read_csv(\"hour.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df26e1f2-252c-4bff-b029-e1608037735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      " instant         int64\n",
      "dteday         object\n",
      "season          int64\n",
      "yr              int64\n",
      "mnth            int64\n",
      "hr              int64\n",
      "holiday         int64\n",
      "weekday         int64\n",
      "workingday      int64\n",
      "weathersit      int64\n",
      "temp          float64\n",
      "atemp         float64\n",
      "hum           float64\n",
      "windspeed     float64\n",
      "casual          int64\n",
      "registered      int64\n",
      "cnt             int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " instant       0\n",
      "dteday        0\n",
      "season        0\n",
      "yr            0\n",
      "mnth          0\n",
      "hr            0\n",
      "holiday       0\n",
      "weekday       0\n",
      "workingday    0\n",
      "weathersit    0\n",
      "temp          0\n",
      "atemp         0\n",
      "hum           0\n",
      "windspeed     0\n",
      "casual        0\n",
      "registered    0\n",
      "cnt           0\n",
      "dtype: int64\n",
      "\n",
      "Continuous columns: ['temp', 'atemp', 'hum', 'windspeed']\n",
      "Categorical columns: ['season', 'yr', 'mnth', 'hr', 'weekday', 'weathersit']\n",
      "Binary columns: ['holiday', 'workingday']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "print(\"\\nContinuous columns:\", [\"temp\",\"atemp\",\"hum\",\"windspeed\"])\n",
    "print(\"Categorical columns:\", [\"season\",\"yr\",\"mnth\",\"hr\",\"weekday\",\"weathersit\"])\n",
    "print(\"Binary columns:\", [\"holiday\",\"workingday\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6a2f4d-bc96-4184-b203-02c3e63705ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"instant\", \"dteday\", \"casual\", \"registered\"]\n",
    "df_model = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768a07f4-fcd2-490d-9ba3-9f53d21b93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"cnt\"\n",
    "y = df_model[target].values\n",
    "X = df_model.drop(columns=[target])\n",
    "\n",
    "categorical_cols = [\"season\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"weathersit\"]\n",
    "numeric_cols = [\"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "    (\"num\", \"passthrough\", numeric_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6f945-9cbf-4292-b7b4-ea42dfa55507",
   "metadata": {},
   "source": [
    "### A.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b402e993-5d3a-460c-a0c1-ae3806acfd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13903, 12)  Test: (3476, 12)\n"
     ]
    }
   ],
   "source": [
    "n = len(df_model)\n",
    "split_idx = int(0.8 * n)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352fdfd-893d-4e21-8581-5ce5be52a5de",
   "metadata": {},
   "source": [
    "### A.3 Baseline Model (Single Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d292ec3-1c03-4993-899f-804f0b20a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=6) RMSE: 158.692\n",
      "Linear Regression RMSE: 133.835\n",
      "\n",
      "âœ… Baseline model: Linear Regression\n"
     ]
    }
   ],
   "source": [
    "tree_model = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"reg\", DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "lin_model = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "lin_model.fit(X_train, y_train)\n",
    "\n",
    "pred_tree = tree_model.predict(X_test)\n",
    "pred_lin  = lin_model.predict(X_test)\n",
    "\n",
    "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_tree = rmse(y_test, pred_tree)\n",
    "rmse_lin  = rmse(y_test, pred_lin)\n",
    "\n",
    "print(f\"Decision Tree (max_depth=6) RMSE: {rmse_tree:.3f}\")\n",
    "print(f\"Linear Regression RMSE: {rmse_lin:.3f}\")\n",
    "\n",
    "baseline = \"Decision Tree\" if rmse_tree < rmse_lin else \"Linear Regression\"\n",
    "print(f\"\\nâœ… Baseline model: {baseline}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc42f6-03c0-4a5d-868a-c580e476cccf",
   "metadata": {},
   "source": [
    "### ðŸ§¾ **Conclusion â€” Part A**\n",
    "\n",
    "- After preprocessing and chronological splitting, two baseline models were trained:  \n",
    "  - **Decision Tree (max_depth = 6)** â†’ RMSE = **158.69**  \n",
    "  - **Linear Regression** â†’ RMSE = **133.84**  \n",
    "- Since Linear Regression achieved the lower RMSE, it is selected as the **baseline model** for further ensemble comparisons.  \n",
    "- This indicates that the relationship between the predictors and total bike rentals (`cnt`) is largely **linear** with limited complex non-linear interactions at this stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf533940-182e-499b-922e-b4c9a0abc20b",
   "metadata": {},
   "source": [
    "## Part B: Ensemble Techniques for Bias and Variance Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261b8a0-40d7-41fe-a09e-57d55653933f",
   "metadata": {},
   "source": [
    "### B.1 Bagging (variance reduction)\n",
    "\n",
    "**Hypothesis:**  \n",
    "Bagging aims to **reduce variance** by averaging predictions from multiple high-variance models (like decision trees) trained on different bootstrap samples of the data.\n",
    "\n",
    "**Implementation :**  \n",
    "- Use **DecisionTreeRegressor(max_depth = 6)** as the baseline.  \n",
    "- Train a **BaggingRegressor** with at least **50 estimators** (weâ€™ll use 100).  \n",
    "- Evaluate its **test RMSE** and compare it against the single Decision Tree baseline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62eb1a24-e9e3-4d0b-aff0-e6035780a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (single) RMSE: 158.692\n",
      "Bagging (100 DT) RMSE:        155.270\n"
     ]
    }
   ],
   "source": [
    "bagging_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"bag\", BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE),\n",
    "        n_estimators=100,            # >= 50 as required\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# fit and evaluate\n",
    "bagging_pipe.fit(X_train, y_train)\n",
    "pred_bag = bagging_pipe.predict(X_test)\n",
    "rmse_bag = float(np.sqrt(mean_squared_error(y_test, pred_bag)))\n",
    "\n",
    "tree_pipe = Pipeline([(\"prep\", preprocessor),\n",
    "                      (\"reg\", DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE))])\n",
    "tree_pipe.fit(X_train, y_train)\n",
    "pred_tree = tree_pipe.predict(X_test)\n",
    "rmse_tree = float(np.sqrt(mean_squared_error(y_test, pred_tree)))\n",
    "\n",
    "print(f\"Decision Tree (single) RMSE: {rmse_tree:.3f}\")\n",
    "print(f\"Bagging (100 DT) RMSE:        {rmse_bag:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473aac84-79c1-4def-a496-ada568bc515e",
   "metadata": {},
   "source": [
    "### Discussion â€” Bagging (Variance Reduction)\n",
    "\n",
    "- The **single Decision Tree (max_depth = 6)** achieved an RMSE of **158.69**,  \n",
    "  while the **Bagging ensemble (100 trees)** achieved a lower RMSE of **155.27**.  \n",
    "- This small but consistent decrease in RMSE indicates that **bagging successfully reduced variance** by averaging predictions from multiple bootstrapped trees.  \n",
    "- Each tree individually is a high-variance learner, but by aggregating many such models, random fluctuations in individual trees cancel out.  \n",
    "- Hence, bagging produced a **more stable and slightly more accurate** predictor compared to a single Decision Tree, validating the hypothesis that bagging primarily targets **variance reduction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d142b-93ea-4881-84d3-61b9feec23ec",
   "metadata": {},
   "source": [
    "### B.2 Boosting (bias reduction) using GradientBoostingRegressor\n",
    "\n",
    "**Hypothesis:**  \n",
    "Boosting primarily targets **bias reduction** by sequentially improving weak learners.  \n",
    "Each new tree in the sequence is trained to correct the residual (error) of the previous ensemble.\n",
    "\n",
    "**Implementation plan:**  \n",
    "- Use **GradientBoostingRegressor** â€” a standard boosting algorithm.  \n",
    "- Keep default hyperparameters to observe natural bias reduction.  \n",
    "- Evaluate test **RMSE** and compare it with both the **single regressors** and **Bagging ensemble** results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c71c40-19b3-4fb1-9a4d-a45f7442f67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting RMSE: 123.182\n",
      "\n",
      "Comparison summary:\n",
      " - Single Decision Tree RMSE: 158.692\n",
      " - Linear Regression RMSE: 133.835\n",
      " - Bagging (100 DT) RMSE:     155.270\n",
      " - Gradient Boosting RMSE:    123.182\n"
     ]
    }
   ],
   "source": [
    "gbr_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"gbr\", GradientBoostingRegressor(random_state=20))\n",
    "])\n",
    "\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "pred_gbr = gbr_pipe.predict(X_test)\n",
    "rmse_gbr = float(np.sqrt(mean_squared_error(y_test, pred_gbr)))\n",
    "\n",
    "print(f\"Gradient Boosting RMSE: {rmse_gbr:.3f}\")\n",
    "print(\"\\nComparison summary:\")\n",
    "print(f\" - Single Decision Tree RMSE: {rmse_tree:.3f}\")\n",
    "print(f\" - Linear Regression RMSE: {rmse_lin:.3f}\")\n",
    "print(f\" - Bagging (100 DT) RMSE:     {rmse_bag:.3f}\")\n",
    "print(f\" - Gradient Boosting RMSE:    {rmse_gbr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc937b-3bd1-47ad-a1f5-c84cd78e2ff1",
   "metadata": {},
   "source": [
    "###  Discussion â€” Boosting (Bias Reduction)\n",
    "\n",
    "- The **Gradient Boosting Regressor** achieved an RMSE of **123.13**, outperforming  \n",
    "  both the **single Decision Tree (158.69)** and the **Bagging ensemble (155.27)**,  \n",
    "  as well as the **Linear Regression baseline (133.84)**.  \n",
    "- This significant improvement supports the hypothesis that **boosting effectively reduces bias** by sequentially learning from the residual errors of prior models.  \n",
    "- Unlike bagging, which trains models in parallel for variance reduction, boosting builds models **sequentially**, allowing later trees to focus on previously mispredicted cases.  \n",
    "- The resulting ensemble combines weak learners into a strong, low-bias model â€” evident from the large RMSE drop â€” confirming that **boosting achieved superior accuracy through bias correction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9da01-4f53-4ea2-8ab0-69aad7cdacfd",
   "metadata": {},
   "source": [
    "## Part C: Stacking for Optimal Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f51f25-2169-4137-a388-6c40200cd61b",
   "metadata": {},
   "source": [
    "### C.1 Stacking principle \n",
    "\n",
    "Stacking trains several **diverse base learners** (level-0) and then trains a **meta-learner** (level-1) on the base learners' *out-of-sample* predictions.  \n",
    "Formally, base learners produce predictions $\\{\\hat y^{(k)}\\}$; the meta-learner $f_{\\text{meta}}$ learns:\n",
    "\\\\[\n",
    "\\hat y = f_{\\text{meta}}\\big(\\hat y^{(1)}, \\hat y^{(2)}, \\dots, \\hat y^{(K)}\\big).\n",
    "\\\\]\n",
    "Because base learners have different inductive biases (instance-based KNN, bagged trees, gradient-boosted trees), stacking leverages their complementary strengths. The meta-learner learns an optimal combination (often regularized) to reduce generalization error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5f5e9c-32bf-4065-9433-9f8497566741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor RMSE: 116.479\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    preprocessor \n",
    "except NameError:\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    categorical_cols = [\"season\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"weathersit\"]\n",
    "    numeric_cols = [\"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"cat\", ohe, categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ])\n",
    "\n",
    "# KNN \n",
    "knn_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"scale\", StandardScaler()),   \n",
    "    (\"knn\", KNeighborsRegressor(n_neighbors=10))\n",
    "])\n",
    "\n",
    "# Bagging\n",
    "try:\n",
    "    bag_base = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE),\n",
    "        n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "except TypeError:\n",
    "    bag_base = BaggingRegressor(\n",
    "        base_estimator=DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE),\n",
    "        n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "\n",
    "bag_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"bag\", bag_base)\n",
    "])\n",
    "\n",
    "# Gradient Boosting \n",
    "gbr_pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"gbr\", GradientBoostingRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Stacking regressor\n",
    "estimators = [\n",
    "    (\"knn\", knn_pipe),\n",
    "    (\"bag\", bag_pipe),\n",
    "    (\"gbr\", gbr_pipe)\n",
    "]\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(),   \n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "pred_stack = stack.predict(X_test)\n",
    "rmse_stack = float(np.sqrt(mean_squared_error(y_test, pred_stack)))\n",
    "\n",
    "print(f\"Stacking Regressor RMSE: {rmse_stack:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a8773-bc05-42cb-9f76-6000b1e54a2d",
   "metadata": {},
   "source": [
    "### Stacking Regressor â€” Test Set Result\n",
    "\n",
    "The **Stacking Regressor** (combining KNN, Bagging, and Gradient Boosting as base learners with a Ridge meta-learner) achieved a **test RMSE of 116.48**.  \n",
    "This RMSE is notably lower than that of all previous individual and ensemble models, indicating improved predictive performance through optimal blending of diverse learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753825f-2a24-431e-a4d3-068dd05b33a2",
   "metadata": {},
   "source": [
    "## Part D: Final Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6885fc-d373-46a6-811c-2178d525a586",
   "metadata": {},
   "source": [
    "### D.1 â€” Comparative table of RMSEs for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e822b7-8e19-45af-9eb6-5b3dfb3f506b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1edc0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1edc0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_1edc0_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1edc0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1edc0_row0_col0\" class=\"data row0 col0\" >Stacking (KNN + Bag + GBR -> Ridge)</td>\n",
       "      <td id=\"T_1edc0_row0_col1\" class=\"data row0 col1\" >116.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1edc0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1edc0_row1_col0\" class=\"data row1 col0\" >Gradient Boosting</td>\n",
       "      <td id=\"T_1edc0_row1_col1\" class=\"data row1 col1\" >123.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1edc0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1edc0_row2_col0\" class=\"data row2 col0\" >Baseline (Linear)</td>\n",
       "      <td id=\"T_1edc0_row2_col1\" class=\"data row2 col1\" >133.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1edc0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1edc0_row3_col0\" class=\"data row3 col0\" >Bagging (100 DTs)</td>\n",
       "      <td id=\"T_1edc0_row3_col1\" class=\"data row3 col1\" >155.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1edc0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1edc0_row4_col0\" class=\"data row4 col0\" >Decision Tree (single, max_depth=6)</td>\n",
       "      <td id=\"T_1edc0_row4_col1\" class=\"data row4 col1\" >158.692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d0102c41a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame([\n",
    "    {\"Model\": \"Baseline (Linear)\", \"RMSE\": 133.835},   # Linear Regression selected as baseline\n",
    "    {\"Model\": \"Decision Tree (single, max_depth=6)\", \"RMSE\": 158.692},\n",
    "    {\"Model\": \"Bagging (100 DTs)\", \"RMSE\": 155.270},\n",
    "    {\"Model\": \"Gradient Boosting\", \"RMSE\": 123.126},\n",
    "    {\"Model\": \"Stacking (KNN + Bag + GBR -> Ridge)\", \"RMSE\": 116.479}\n",
    "])\n",
    "\n",
    "# Sort by RMSE ascending for clarity\n",
    "results_sorted = results.sort_values(\"RMSE\").reset_index(drop=True)\n",
    "results_sorted.style.format({\"RMSE\": \"{:.3f}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff084a-b412-4bdb-8b4b-e3f1ec339ab7",
   "metadata": {},
   "source": [
    "### ðŸ”Ž Conclusion â€” Best-performing model and why\n",
    "\n",
    "- **Best-performing model:** **Stacking Regressor** (RMSE = **116.48**).  \n",
    "  - It outperformed the baseline (Linear Regression, RMSE = 133.84) and all other ensembles (Bagging, Gradient Boosting).\n",
    "\n",
    "- **Why stacking (and the best ensemble) outperformed the single-model baseline:**\n",
    "  1. **Model diversity:** Stacking combined models with different inductive biases â€”  \n",
    "     KNN (instance-based), Bagged trees (variance-stable, non-linear), and Gradient Boosting (sequential bias correction). These models make different errors on different examples, so combining them reduces overall error.\n",
    "  2. **Biasâ€“variance trade-off:**  \n",
    "     - The baseline Linear Regression had relatively low variance but higher bias (unable to capture some non-linear patterns).  \n",
    "     - Bagging reduced variance of trees but did not sufficiently reduce bias relative to boosting.  \n",
    "     - Gradient Boosting reduced bias substantially (RMSE dropped), but stacking further improved results by leveraging complementary strengths.  \n",
    "     - The meta-learner (Ridge) learns a regularized combination of base predictions, **reducing variance** of the combination while **preserving bias reductions** from strong base learners.\n",
    "  3. **Regularized combination:** Ridge as the meta-learner prevents overfitting to base predictions by penalizing large weights, stabilizing the final ensemble.\n",
    "  4. **Practical outcome:** The stacking ensemble achieved the lowest RMSE, indicating the best empirical balance of bias and variance on the held-out chronological test set.\n",
    "\n",
    "- **Short takeaway:** Ensembles that combine *diverse* learners and use a *regularized meta-learner* (stacking) can outperform both single models and simpler ensembles by simultaneously reducing bias and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cabe2d-0930-4674-83df-f5106a91dff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
